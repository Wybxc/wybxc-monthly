# 文章 | 密码学基础：前置知识

:::tip
新学期选修密码学，第一节课记了这些笔记，于是退课。
:::

## 离散随机变量

对于给定的有限集合 $\Omega=\{a_1,a_2,\cdots,a_n\}$，如果变量 $X$ 满足 $\forall a\in\Omega,\mathrm{Pr}[X=a]=p(a)$，其中 $p(x)$ 是映射 $p(x): \Omega\to [0,1]$，且满足 $\sum_{a\in\Omega}p(a)=1$，那么称 $X$ 是 $\Omega$ 上的一个服从**概率分布（distribution）** $p$ 的**离散随机变量（random variable）**。

对概率分布 $p$，可以定义其**熵（entropy）**：$H(p)=\sum_{a\in\Omega}p(a)\log \frac{1}{p(a)}$，这也被称为随机变量 $X$ 的熵 $H(X)$。

根据函数 $f(x)=x\log \frac{1}{x}$ 的凸性，可知当概率分布中各项的概率相等时，即 $p(a)=\frac{1}{|\Omega|}$，熵达到最大值 $H(p)=\log |\Omega|$。这样的分布称为**均匀分布（uniform distribution）**。

对于多个离散随机变量，可以定义**联合分布（joint distribution）**。如果 $p(x,y): \Omega_1\times\Omega_2\to[0,1]$ 满足 $\forall a\in\Omega_1,b\in\Omega_2, \mathrm{Pr}[X=a,Y=b]=p(a,b)$，那么称 $X, Y$ 服从联合分布 $p$，记作 $(X,Y)\sim p$。

同样地，可以定义多个随机变量的**联合熵（joint entropy）**：若$(X,Y)\sim p$，定义 $H(X,Y)=\sum_{\substack{a\in\Omega_1\\b\in\Omega_2}}p(a,b)\log \frac{1}{p(a,b)}$。

给定联合分布后，可以重新计算每个变量单独的分布，称为这个变量的**边缘分布（marginal distribution）**：$p_1(a)=\sum_{b\in\Omega_2}p(a,b)$，$p_2(b)=\sum_{a\in\Omega_1}p(a,b)$。

如果联合分布可以表示为边缘分布的乘积，称这两个随机变量**独立（independent）**。

当两个变量 $X,Y$ 独立时，计算得
$$
\begin{align}
H(X,Y)&=\sum_{\substack{a\in\Omega_1\\b\in\Omega_2}}p(a,b)\log \frac{1}{p(a,b)}\\
&=\sum_{\substack{a\in\Omega_1\\b\in\Omega_2}}p(a)p(b)\left(\log \frac{1}{p(a)}+\log \frac{1}{p(b)}\right)\\
&=\sum_{b\in\Omega_2}p(b)\sum_{a\in\Omega_1}p(a)\log\frac{1}{p(a)}+\sum_{a\in\Omega_1}p(a)\sum_{b\in\Omega_2}p(b)\log\frac{1}{p(b)}\\
&=H(X)+H(Y)
\end{align}\\
$$

## 条件熵

通过条件概率，可以定义一个随机变量 $X$ 关于随机事件 $e$ 的**条件熵（conditional entropy）**：$H(X|e)=\sum_{a\in\Omega}\mathrm{Pr}[X=a|e]\log\frac{1}{\mathrm{Pr}[X=a|e]}$。

随机变量 $X$ 相对于随机变量 $Y$ 的条件熵等于 $X$ 关于 $Y$ 的某一取值的条件熵在 $Y$ 的边缘分布下的期望：$H(X|Y)=\sum_{b\in\Omega_2}\mathrm{Pr}[Y=b]H(X|Y=b)$。

对上式变形，可以得出条件熵另外的两种表示：
$$
\begin{align}
H(X|Y)&=\sum_{b\in\Omega_2}\mathrm{Pr}[Y=b]H(X|Y=b)\\
&=\sum_{b\in\Omega_2}\mathrm{Pr}[Y=b]\sum_{a\in\Omega_1}\mathrm{Pr}[X=a|Y=b]\log\frac{1}{\mathrm{Pr}[X=a|Y=b]}\\
&=\sum_{\substack{a\in\Omega_1\\b\in\Omega_2}}\mathrm{Pr}[X=a,Y=b]\log\frac{\mathrm{Pr}[Y=b]}{\mathrm{Pr}[X=a,Y=b]}\\
\end{align}\\
$$

$$
\begin{align}
H(X|Y)&=\sum_{\substack{a\in\Omega_1\\b\in\Omega_2}}\mathrm{Pr}[X=a,Y=b]\log\frac{\mathrm{Pr}[Y=b]}{\mathrm{Pr}[X=a,Y=b]}\\
&=\sum_{\substack{a\in\Omega_1\\b\in\Omega_2}}\mathrm{Pr}[X=a,Y=b]\log\frac{1}{\mathrm{Pr}[X=a,Y=b]}-\sum_{b\in\Omega_2}\left(\sum_{a\in\Omega_1}\mathrm{Pr}[X=a,Y=b]\right)\log\frac{1}{\mathrm{Pr}[Y=b]}\\
&=H(X,Y)-\sum_{b\in\Omega_2}\mathrm{Pr}[Y=b]\log\frac{1}{\mathrm{Pr}[Y=b]}\\
&=H(X,Y)-H(Y)\\
\end{align}\\
$$

## 互信息

**互信息（mutual information）**用于衡量两个随机变量的相关性。其定义为以一个随机变量为条件后，熵的减少值。
$$
I(X;Y)=H(X)-H(X|Y)\\
$$
代入 $H(X|Y)=H(X,Y)-H(X)$ ，得到
$$
I(X;Y)=H(X)+H(Y)-H(X,Y)\\
$$
从这里可以看出互信息是对称的。因此有
$$
\begin{align}
I(X;Y)&=H(X)-H(X|Y)=H(Y)-H(Y|X)\\
&=H(X)+H(Y)-H(X,Y)\\
\end{align}\\
$$
进一步变形，可以得到
$$
\begin{align}
I(X;Y)&=\sum_{a\in\Omega_1}p_1(a)\log\frac{1}{p_1(a)}+\sum_{b\in\Omega_2}p_2(b)\log\frac{1}{p_2(b)}-\sum_{\substack{a\in\Omega_1\\b\in\Omega_2}}p(a,b)\log\frac{1}{p(a,b)}\\
&=\sum_{a\in\Omega_1}\left(\sum_{b\in\Omega_2}p(a,b)\right)\log\frac{1}{p_1(a)}+\sum_{b\in\Omega_2}\left(\sum_{a\in\Omega_1}p(a,b)\right)\log\frac{1}{p_2(b)}-\sum_{\substack{a\in\Omega_1\\b\in\Omega_2}}p(a,b)\log\frac{1}{p(a,b)}\\
&=\sum_{\substack{a\in\Omega_1\\b\in\Omega_2}}p(a,b)\log\frac{p(a,b)}{p(a)p(b)}\\
\end{align}\\
$$
于是可以证明互信息的非负性：
$$
\begin{align}
I(X;Y)&=\sum_{\substack{a\in\Omega_1\\b\in\Omega_2}}p(a,b)\log\frac{p(a,b)}{p(a)p(b)}\\
&=-\sum_{\substack{a\in\Omega_1\\b\in\Omega_2}}p(a,b)\log\frac{p(a)p(b)}{p(a,b)}\\
&\ge-\log\sum_{\substack{a\in\Omega_1\\b\in\Omega_2}}p(a,b)\frac{p(a)p(b)}{p(a,b)}\\
&=0
\end{align}\\
$$
由此可知 $H(X|Y)\le H(X)$。

当随机变量 $X$ 与 $Y$ 独立时，有 $H(X,Y)=H(X)+H(Y)$，可知 $H(X|Y)=H(X), H(Y|X)=H(Y), I(X;Y)=0$。

反过来，当互信息 $I(X;Y)=0$ 时，从上面不等式的取等条件可以看出，$\frac{p(a)p(b)}{p(a,b)}$ 不依赖于 $a,b$ 的选取，进而可知 $X$ 与 $Y$ 独立。

当随机变量 $Y$ 完全依赖于 $X$ 时，也即 $\mathrm{Pr}[X=a,Y=b]=\begin{cases}1, &b=f(a)\\0, &\text{otherwise}\end{cases}$ ，有
$$
\begin{align}
H(X,Y)&=\sum_{\substack{a\in\Omega\\b\in f(\Omega)}}p(a,b)\log\frac{1}{p(a,b)}\\
&=\sum_{a\in\Omega}p(a,f(a))\log\frac{1}{p(a,f(a))}\\
&=\sum_{a\in\Omega}p(a)\log\frac{1}{p(a)}\\
&=H(X)\\
\end{align}\\
$$
因此，$I(X;Y)=H(Y)$。

## 多元互信息

对于多于两个的随机变量 $X_1,X_2,\cdots,X_n$，记其概率分布为 $p:\Omega_1\times\Omega_2\times\cdots\times\Omega_n\to[0,1]$，以及每个变量的边缘分布 $X_i\sim p_i$，于是可以定义多元联合熵
$$
H(X_1,X_2,\cdots,X_n)=\sum_{x_1\in\Omega_1}\sum_{x_2\in\Omega_2}\cdots\sum_{x_n\in\Omega_n} p(x_1,x_2\cdots,x_n)\log\frac{1}{p(x_1,x_2\cdots,x_n)}\\
$$
以及关于随机事件 $e$ 的多元条件熵
$$
H(X_1,X_2,\cdots,X_n|e)=\sum_{x_1\in\Omega_1}\sum_{x_2\in\Omega_2}\cdots\sum_{x_n\in\Omega_n} p(x_1,x_2\cdots,x_n|e)\log\frac{1}{p(x_1,x_2\cdots,x_n|e)}\\
$$
考虑三个随机变量 $X,Y,Z$ 的条件熵，定义
$$
\begin{align}
H(X,Y|Z)&=\sum_z\mathrm{Pr}[Z=z]\cdot H(X,Y|Z=z)\\
&=\sum_x\sum_y\sum_z\mathrm{Pr}[Z=z]\cdot\mathrm{Pr}[X=x,Y=y|Z=z]\log\frac{1}{\mathrm{Pr}[X=x,Y=y|Z=z]}\\
&=\sum_x\sum_y\sum_z\mathrm{Pr}[X=x,Y=y,Z=z]\left(\log\frac{1}{\mathrm{Pr}[X=x,Y=y,Z=z]}-\log\frac{1}{\mathrm{Pr}[Z=z]}\right)\\
&=H(X,Y,Z)-\sum_z\left(\sum_x\sum_y\mathrm{Pr}[X=x,Y=y,Z=z]\right)\log\frac{1}{\mathrm{Pr}[Z=z]}\\
&=H(X,Y,Z)-\sum_z \mathrm{Pr}[Z=z]\log\frac{1}{\mathrm{Pr}[Z=z]}\\
&=H(X,Y,Z)-H(Z)\\
\end{align}\\
$$
接下来定义关于随机事件 $e$ 的条件互信息
$$
I(X;Y|e)=H(X|e)+H(Y|e)-H(X,Y|e)\\
$$
以及关于随机变量的条件互信息
$$
\begin{align}
    I(X;Y|Z)&=\sum_{z}\mathrm{Pr}[Z=z]\cdot I(X;Y|Z=z) \\
    &=\sum_z \mathrm{Pr}[Z=z]\cdot \left(H(X|Z=z)+H(Y|Z=z)-H(X,Y|Z=z)\right)\\
    &=H(X|Z)+H(Y|Z)-H(X,Y|Z)
\end{align}
$$
继续定义多元互信息
$$
I(X_1;X_2;\cdots;X_n)=\sum_{i=1}^n H(X_i)-\sum_{1\le i<j\le n}H(X_i,H_j)+\cdots+(-1)^{n-1}H(X_1,X_2,\cdots,X_n)\\
$$
特别地，有三元互信息
$$
\begin{align}
I(X;Y;Z)&=H(X) + H(Y) + H(Z) −H(X, Y) −H(X, Z) −H(Y, Z) + H(X, Y, Z)\\
&=H(X) + H(Y) −H(X, Y) + 2H(Z) −H(X, Z) −H(Y, Z) + H(X, Y|Z)\\
&=I(X;Y) - H(X|Z)-H(Y|Z)+H(X,Y|Z)\\
&=I(X;Y) - I(X;Y|Z)\\
\end{align}\\
$$

仿照二元互信息，可知
$$
I(X;Y|e)=\sum_x\sum_y\mathrm{Pr}[X=x,Y=y|e]\log\frac{\mathrm{Pr}[X=x,Y=y|e]}{\mathrm{Pr}[X=x|e]\cdot\mathrm{Pr}[Y=y|e]}\\
$$
于是
$$
\begin{align}
I(X;Y|Z)&=\sum_x\sum_y\sum_z\mathrm{Pr}[Z=z]\cdot\mathrm{Pr}[X=x,Y=y|Z=z]\cdot\log\frac{\mathrm{Pr}[X=x,Y=y|Z=z]}{\mathrm{Pr}[X=x|Z=z]\cdot\mathrm{Pr}[Y=y|Z=z]}\\
&=\sum_x\sum_y\sum_z\mathrm{Pr}[X=x,Y=y,Z=z]\cdot\left(\log\frac{\mathrm{Pr}[X=x,Y=y,Z=z]}{\mathrm{Pr}[X=x,Z=z]\cdot\mathrm{Pr}[Y=y,Z=z]}-\log\frac{1}{\mathrm{Pr}[Z=z]}\right)\\
&=-H(Z)-\sum_x\sum_y\mathrm{Pr}[X=x,Y=y]\sum_z\frac{\mathrm{Pr}[X=x,Y=y,Z=z]}{\mathrm{Pr}[X=x,Y=y]}\cdot\log\frac{\mathrm{Pr}[X=x,Z=z]\cdot\mathrm{Pr}[Y=y,Z=z]}{\mathrm{Pr}[X=x,Y=y,Z=z]}\\
&\le-H(Z)-\sum_x\sum_y\mathrm{Pr}[X=x,Y=y]\cdot\log\sum_z\frac{\mathrm{Pr}[X=x,Y=y,Z=z]}{\mathrm{Pr}[X=x,Y=y]}\cdot\frac{\mathrm{Pr}[X=x,Z=z]\cdot\mathrm{Pr}[Y=y,Z=z]}{\mathrm{Pr}[X=x,Y=y,Z=z]}\\
&=-H(Z)-\sum_x\sum_y\mathrm{Pr}[X=x,Y=y]\cdot\left(\log\frac{1}{\mathrm{Pr}[X=x,Y=y]}+\log\sum_z\mathrm{Pr}[X=x,Z=z]\cdot\mathrm{Pr}[Y=y,Z=z]\right)\\
\end{align}\\
$$


## 马尔可夫不等式

对实数域上的离散随机变量 $X$，记期望 $E(X)=\sum_{a\in\Omega}ap(a)$，当 $X$ 恒非负时，有**马尔可夫不等式（Markov inequality）**：
$$
\mathrm{Pr}[X\ge a]=\frac{\sum_{\substack{b\in\Omega\\b\ge a}}ap(b)}{a}\le\frac{\sum_{\substack{b\in\Omega\\b\ge a}}bp(b)}{a}\le \frac{\sum_{b\in\Omega}bp(b)}{a}=\frac{\mathbb{E}(X)}{a}\\
$$
如果 $X$ 是另一个随机变量 $Y$ 到其均值的距离的平方，则有**切比雪夫不等式（Chebyshev inequality）**：
$$
\mathrm{Pr}[|Y-\mathbb{E}Y|\ge c]=\mathrm{Pr}[(Y-\mathbb{E}Y)^2\ge c^2]\le\frac{\mathbb{E}((Y-\mathbb{E}Y)^2)}{c^2}=\frac{\mathbb{D}(Y)}{c^2}\\
$$
切比雪夫不等式描述了方差与随机变量偏离均值的程度之间的关系。

对于一般的多个随机变量 $X_1, X_2,\cdots, X_n$，记 $X=\sum_{i=1}^n X_i$，代入切比雪夫不等式有
$$
\mathrm{Pr}\left[\left|\frac{1}{n}\sum_{i=1}^nX_i-\frac{1}{n}\sum_{i=1}^n\mathbb{E}X_i\right|\ge c\right]=\mathrm{Pr}\left[\left|\sum_{i=1}^nX_i-\sum_{i=1}^n\mathbb{E}X_i\right|\ge nc\right]\le\frac{\mathbb{D}(X)}{n^2c^2}=\frac{1}{n^2c^2}\sum_{i=1}^n\mathbb{D}X_i\\
$$
若 $X_1, X_2,\cdots, X_n$ 是同分布的，记均值 $\mu=\mathbb{E}(X_i)$，方差 $\sigma^2=\mathbb{D}(X_i)$，则
$$
\mathrm{Pr}\left[\left|\frac{1}{n}\sum_{i=1}^nX_i-\mu\right|\ge c\right]\le\frac{\sigma^2}{nc^2}\\
$$
令 $n\to\infty$，则
$$
\lim_{n\to\infty}\mathrm{Pr}\left[\left|\frac{1}{n}\sum_{i=1}^nX_i-\mu\right|\ge c\right]=0\\
$$
这就是**弱大数定律（weak law of large numbers）**。

## 切诺夫界

如果对 $X_1, X_2,\cdots, X_n$ 有更多假设，可以给出比弱大数定律更强的估计。

若 $X_1, X_2,\cdots, X_n$ 为独立泊松实验，即 $X_i\in\{0,1\}$，$\mathrm{Pr}[X_i=1]=p_i$，记 $X=\sum_{i=1}^n X_i$ ，$\mu=\mathbb{E}X=\sum_{i=1}^n p_i$，那么
$$
\begin{align}
\mathrm{Pr}\left[\sum_{i=1}^nX_i\ge(1+\delta)\mu\right]&=\mathrm{Pr}\left[e^{tX}\ge e^{(1+\delta)t\mu}\right]\\
&\le\frac{\mathbb{E}(e^{tX})}{e^{(1+\delta)t\mu}}\\
&=\frac{1}{e^{(1+\delta)t\mu}}\prod_{i=1}^n \mathbb{E}(e^{tX_i})\\
&=\frac{1}{e^{(1+\delta)t\mu}}\prod_{i=1}^n (p_ie^t+1-p_i)\\
&\le\frac{1}{e^{(1+\delta)t\mu}}\prod_{i=1}^n \exp(p_i(e^t-1))\\
&=\frac{1}{e^{(1+\delta)t\mu}}\exp\left(\sum_{i=1}^np_i(e^t-1)\right)\\
&=\frac{\exp(\mu(e^t-1))}{\exp((1+\delta)t\mu)}\\
&=\exp\left(\mu(1+\delta)\left(e^{t-\ln(1+\delta)}-\frac{1}{1+\delta}-t\right)\right)
\end{align}\\
$$
令 $t=\ln(1+\delta)$，则
$$
\mathrm{Pr}\left[\sum_{i=1}^nX_i\ge(1+\delta)\mu\right]\le \left(\frac{e^\delta}{(1+\delta)^{1+\delta}}\right)^\mu\\
$$
同理，有
$$
\mathrm{Pr}\left[\sum_{i=1}^nX_i\le(1-\delta)\mu\right]\le \left(\frac{e^{-\delta}}{(1-\delta)^{1-\delta}}\right)^\mu\\
$$
此二式称为**切诺夫界（Chernoff bound）**。